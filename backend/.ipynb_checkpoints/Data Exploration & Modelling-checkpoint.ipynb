{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Alexa Review - Sentiment Analysis\n",
    "\n",
    "Analyzing the Amazon Alexa dataset and building classification models to predict if the sentiment of a given input sentence is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\GEETA\n",
      "[nltk_data]     HOSMANI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "data = pd.read_csv(r\"Data\\amazon_alexa.tsv\", delimiter = '\\t', quoting = 3)\n",
    "\n",
    "print(f\"Dataset shape : {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column names\n",
    "\n",
    "print(f\"Feature names : {data.columns.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one record with no 'verified_reviews' (null value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the record where 'verified_reviews' is null \n",
    "\n",
    "data[data['verified_reviews'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will drop the null record\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape after dropping null values : {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column 'length' that will contain the length of the string in 'verified_reviews' column\n",
    "\n",
    "data['length'] = data['verified_reviews'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'length' column is new generated column - stores the length of 'verified_reviews' for that record. Let's check for some sample records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly checking for 10th record\n",
    "\n",
    "print(f\"'verified_reviews' column value: {data.iloc[10]['verified_reviews']}\") #Original value\n",
    "print(f\"Length of review : {len(data.iloc[10]['verified_reviews'])}\") #Length of review using len()\n",
    "print(f\"'length' column value : {data.iloc[10]['length']}\") #Value of the column 'length'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length of review is the same as the value in the length column for that record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datatypes of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rating, feedback and length are integer values <br>\n",
    "* date, variation and verified_reviews are string values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing 'rating' column\n",
    "\n",
    "This column refers to the rating of the variation given by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct values of 'rating' and its count  \n",
    "\n",
    "print(f\"Rating value count: \\n{data['rating'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the above values in a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot to visualize the total counts of each rating\n",
    "\n",
    "data['rating'].value_counts().plot.bar(color = 'red')\n",
    "plt.title('Rating distribution count')\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the percentage distribution of each rating - we'll divide the number of records for each rating by total number of records\n",
    "\n",
    "print(f\"Rating value count - percentage distribution: \\n{round(data['rating'].value_counts()/data.shape[0]*100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the above values in a pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "colors = ('red', 'green', 'blue','orange','yellow')\n",
    "\n",
    "wp = {'linewidth':1, \"edgecolor\":'black'}\n",
    "\n",
    "tags = data['rating'].value_counts()/data.shape[0]\n",
    "\n",
    "explode=(0.1,0.1,0.1,0.1,0.1)\n",
    "\n",
    "tags.plot(kind='pie', autopct=\"%1.1f%%\", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of rating')\n",
    "\n",
    "from io import  BytesIO\n",
    "\n",
    "graph = BytesIO()\n",
    "\n",
    "fig.savefig(graph, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing 'feedback' column\n",
    "\n",
    "This column refers to the feedback of the verified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct values of 'feedback' and its count \n",
    "\n",
    "print(f\"Feedback value count: \\n{data['feedback'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 distinct values of 'feedback' present - 0 and 1. Let's see what kind of review each value corresponds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feedback value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the 'verified_reviews' value for one record with feedback = 0\n",
    "\n",
    "review_0 = data[data['feedback'] == 0].iloc[1]['verified_reviews']\n",
    "print(review_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the 'verified_reviews' value for one record with feedback = 1\n",
    "\n",
    "review_1 = data[data['feedback'] == 1].iloc[1]['verified_reviews']\n",
    "print(review_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above 2 examples we can see that feedback **0 is negative review** and **1 is positive review**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the feedback value count in a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph to visualize the total counts of each feedback\n",
    "\n",
    "data['feedback'].value_counts().plot.bar(color = 'blue')\n",
    "plt.title('Feedback distribution count')\n",
    "plt.xlabel('Feedback')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the percentage distribution of each feedback - we'll divide the number of records for each feedback by total number of records\n",
    "\n",
    "print(f\"Feedback value count - percentage distribution: \\n{round(data['feedback'].value_counts()/data.shape[0]*100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback distribution <br>\n",
    "* 91.87% reviews are positive <br>\n",
    "* 8.13% reviews are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "colors = ('red', 'green')\n",
    "\n",
    "wp = {'linewidth':1, \"edgecolor\":'black'}\n",
    "\n",
    "tags = data['feedback'].value_counts()/data.shape[0]\n",
    "\n",
    "explode=(0.1,0.1)\n",
    "\n",
    "tags.plot(kind='pie', autopct=\"%1.1f%%\", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of feedback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the 'rating' values for different values of 'feedback'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feedback = 0\n",
    "data[data['feedback'] == 0]['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feedback = 1\n",
    "data[data['feedback'] == 1]['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If rating of a review is 1 or 2 then the feedback is 0 (negative) and if the rating is 3, 4 or 5 then the feedback is 1 (positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing 'variation' column\n",
    "\n",
    "This column refers to the variation or type of Amazon Alexa product. Example - Black Dot, Charcoal Fabric etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct values of 'variation' and its count \n",
    "\n",
    "print(f\"Variation value count: \\n{data['variation'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph to visualize the total counts of each variation\n",
    "\n",
    "data['variation'].value_counts().plot.bar(color = 'orange')\n",
    "plt.title('Variation distribution count')\n",
    "plt.xlabel('Variation')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the percentage distribution of each variation - we'll divide the number of records for each variation by total number of records\n",
    "\n",
    "print(f\"Variation value count - percentage distribution: \\n{round(data['variation'].value_counts()/data.shape[0]*100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean rating according to variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('variation')['rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the above ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('variation')['rating'].mean().sort_values().plot.bar(color = 'brown', figsize=(11, 6))\n",
    "plt.title(\"Mean rating according to variation\")\n",
    "plt.xlabel('Variation')\n",
    "plt.ylabel('Mean rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing 'verified_reviews' column\n",
    "\n",
    "This column contains the textual review given by the user for a variation for the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length analysis for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['length'],color='blue').set(title='Distribution of length of review ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length analysis when feedback is 0 (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data[data['feedback']==0]['length'],color='red').set(title='Distribution of length of review if feedback = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length analysis when feedback is 1 (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data[data['feedback']==1]['length'],color='green').set(title='Distribution of length of review if feedback = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lengthwise mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('length')['rating'].mean().plot.hist(color = 'blue', figsize=(7, 6), bins = 20)\n",
    "plt.title(\" Review length wise mean ratings\")\n",
    "plt.xlabel('ratings')\n",
    "plt.ylabel('length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "words = cv.fit_transform(data.verified_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews\n",
    "reviews = \" \".join([review for review in data['verified_reviews']])\n",
    "                        \n",
    "# Initialize wordcloud object\n",
    "wc = WordCloud(background_color='white', max_words=50)\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc.generate(reviews))\n",
    "plt.title('Wordcloud for all reviews', fontsize=10)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the unique words in each feedback category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews for each feedback category and splitting them into individual words\n",
    "neg_reviews = \" \".join([review for review in data[data['feedback'] == 0]['verified_reviews']])\n",
    "neg_reviews = neg_reviews.lower().split()\n",
    "\n",
    "pos_reviews = \" \".join([review for review in data[data['feedback'] == 1]['verified_reviews']])\n",
    "pos_reviews = pos_reviews.lower().split()\n",
    "\n",
    "#Finding words from reviews which are present in that feedback category only\n",
    "unique_negative = [x for x in neg_reviews if x not in pos_reviews]\n",
    "unique_negative = \" \".join(unique_negative)\n",
    "\n",
    "unique_positive = [x for x in pos_reviews if x not in neg_reviews]\n",
    "unique_positive = \" \".join(unique_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', max_words=50)\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc.generate(unique_negative))\n",
    "plt.title('Wordcloud for negative reviews', fontsize=10)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative words can be seen in the above word cloud - garbage, pointless, poor, horrible, repair etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', max_words=50)\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc.generate(unique_positive))\n",
    "plt.title('Wordcloud for positive reviews', fontsize=10)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive words can be seen in the above word cloud - good, enjoying, amazing, best, great etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the corpus from the 'verified_reviews' we perform the following - <br>\n",
    "1. Replace any non alphabet characters with a space\n",
    "2. Covert to lower case and split into words\n",
    "3. Iterate over the individual words and if it is not a stopword then add the stemmed form of the word to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(0, data.shape[0]):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', data.iloc[i]['verified_reviews'])\n",
    "  review = review.lower().split()\n",
    "  review = [stemmer.stem(word) for word in review if not word in STOPWORDS]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Count Vectorizer to create bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 2500)\n",
    "\n",
    "#Storing independent and dependent variables in X and y\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = data['feedback'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Count Vectorizer\n",
    "pickle.dump(cv, open('Models/countVectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into train and test set with 30% data with testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 15)\n",
    "\n",
    "print(f\"X train: {X_train.shape}\")\n",
    "print(f\"y train: {y_train.shape}\")\n",
    "print(f\"X test: {X_test.shape}\")\n",
    "print(f\"y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X train max value: {X_train.max()}\")\n",
    "print(f\"X test max value: {X_test.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll scale X_train and X_test so that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the scaler model\n",
    "pickle.dump(scaler, open('Models/scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting scaled X_train and y_train on Random Forest Classifier\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of the model on training and testing data\n",
    " \n",
    "print(\"Training Accuracy :\", model_rf.score(X_train_scl, y_train))\n",
    "print(\"Testing Accuracy :\", model_rf.score(X_test_scl, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on the test set\n",
    "y_preds = model_rf.predict(X_test_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model_rf.classes_)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(estimator = model_rf, X = X_train_scl, y = y_train, cv = 10)\n",
    "\n",
    "print(\"Accuracy :\", accuracies.mean())\n",
    "print(\"Standard Variance :\", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying grid search to get the optimal parameters on random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 100],\n",
    "    'min_samples_split': [8, 12],\n",
    "    'n_estimators': [100, 300]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_object = StratifiedKFold(n_splits = 2)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model_rf, param_grid = params, cv = cv_object, verbose = 0, return_train_score = True)\n",
    "grid_search.fit(X_train_scl, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the best parameters from the grid search\n",
    "\n",
    "\n",
    "print(\"Best Parameter Combination : {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation mean accuracy on train set : {}\".format(grid_search.cv_results_['mean_train_score'].mean()*100))\n",
    "print(\"Cross validation mean accuracy on test set : {}\".format(grid_search.cv_results_['mean_test_score'].mean()*100))\n",
    "print(\"Accuracy score for test set :\", accuracy_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of the model on training and testing data\n",
    " \n",
    "print(\"Training Accuracy :\", model_xgb.score(X_train_scl, y_train))\n",
    "print(\"Testing Accuracy :\", model_xgb.score(X_test_scl, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model_xgb.classes_)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the XGBoost classifier\n",
    "pickle.dump(model_xgb, open('Models/model_xgb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeClassifier()\n",
    "model_dt.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of the model on training and testing data\n",
    " \n",
    "print(\"Training Accuracy :\", model_dt.score(X_train_scl, y_train))\n",
    "print(\"Testing Accuracy :\", model_dt.score(X_test_scl, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model_dt.classes_)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
